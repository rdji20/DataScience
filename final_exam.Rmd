---
title: "IMT 573 Final Exam"
author: "Roberto Infante"
date: 'Due: Aug 25, 2023'
output: pdf_document
header-includes:
- \newcommand{\benum}{\begin{enumerate}}
- \newcommand{\eenum}{\end{enumerate}}
- \newcommand{\bitem}{\begin{itemize}}
- \newcommand{\eitem}{\end{itemize}}
---

\noindent {\bf Instructions}

This is a take-home final examination. You may use your computer, books/articles, notes, course materials, etc., but all work must be your own! References must be appropriately cited. Please justify your answers and show all work; a complete argument must be presented to obtain full credit. Before beginning this exam, please ensure you have access to R and RStudio; this can be on your own personal computer or on the IMT 573 R Studio Server. 

1. Open the `final_exam.Rmd` file on RStudio Cloud. Supply your solutions to the exam by editing `final_exam.Rmd`. 

2. Replace the "Insert Your Name Here" text in the `author:` field with your own full name.

3. Be sure to include well-documented (e.g. commented) code chucks, figures, and clearly written text chunk explanations as necessary. Any figures should be clearly labeled and appropriately referenced within the text. Be sure that each visualization adds value to your written explanation; avoid redundancy -- you do not need four different visualizations of the same pattern.

4.  \textbf{Collaboration is not allowed on this exam.} You may only speak with the Prof. Ernest Green about this material. 

5. All materials and resources that you use (with the exception of lecture slides) must be appropriately referenced within your assignment.  

6. Remember partial credit will be awarded for each question for which a serious attempt at finding an answer has been shown. Students are \emph{strongly} encouraged to attempt each question and to document their reasoning process even if they cannot find the correct answer. If you would like to include R code to show this process, but it does not run without errors, you can do so with the `eval=FALSE` option. (Note: I am also using the `include=FALSE` option here to not include this code in the PDF, but you need to remove this or change it to `TRUE` if you want to include the code chunk.)

```{r example chunk with a bug, eval=FALSE, include=FALSE}
a + b # these object dont' exist 
# if you run this on its own it with give an error
```

7. When you have completed the assignment and have **checked** that your code both runs in the Console and knits correctly when you click `Knit PDF`, rename the knitted PDF file to `YourLastName_YourFirstName.pdf`, and submit your PDF file on Canvas.

\noindent {\bf Statement of Compliance} 

You \textbf{must} include the a ``signed'' Statement of Compliance in your submission. The Compliance Statement is found on the next page of this exam. You must include this text, word-for-word, in your final exam submission. Adding your name indicates you have read the statement and agree to its terms. Failure to do so will result in your exam \textbf{not} being accepted.

\newpage

\begin{center}
{\bf Statement of Compliance}
\end{center}

\noindent I affirm that I have had no conversation regarding this exam with any persons other than the instructor (Dr. Emma Spiro). Further, I certify that the attached work represents my own thinking. Any information, concepts, or words that originate from other sources are cited in accordance with University of Washington guidelines as published in the Academic Code (available on the course website). I am aware of the serious consequences that result from improper discussions with others or from the improper citation of work that is not my own. 
\vspace{.1in}

\noindent (you name goes here as signature) 

\noindent (date of signature)

\newpage

\noindent {\bf Setup}

In this exam you will need, at minimum, the following R packages.

```{r Setup, message=FALSE}
library(tidyverse)
library(gridExtra)
library(dplyr)
library(ggplot2)
library(MASS)
library(boot)
library(bestglm)
library(tree)
library(randomForest)
library(pROC)
library(AER)
library(car)
```

\newpage

\noindent {\bf Problem 1} \hfill (15 pts)
\vspace{.25in}

\noindent In this problem we will use the infidelity data, known as the Fair's Affairs dataset. The `Affairs` dataset is available as part of the \texttt{AER} package in \textbf{R}. This data comes from a survey conducted by \emph{Psychology Today} in 1969, see Greene (2003) and Fair (1978) for more information. 

```{r}
data(Affairs) # load data
str(Affairs) # explore structure of data
```

\noindent The dataset contains various self-reported characteristics of 601 participants, including how often the respondent engaged in extramarital sexual intercourse during the past year, as well as their gender, age, year married, whether they had children, their religiousness (on a 5-point scale, from 1=anti to 5=very), education, occupation (Hillingshead 7-point classification with reverse numbering), and a numeric self-rating of their marriage (from 1=very unhappy to 5=very happy).

\bitem
\item[(a)] Describe the participants. Use descriptive, summarization, and exploratory techniques to describe the participants in the study. For example, what proportion of respondents are female? What is the average age of respondents? In your response comment on any ethical and privacy concerns you have with this dataset.

**Exploratory Data Analysis**

```{r}
summary(Affairs)
```

```{r}
ggplot(Affairs, aes(x = affairs)) +
  geom_histogram(binwidth = 1) +
  labs(title = "Distribution of Number of Affairs",
       x = "Number of Affairs",
       y = "Count") +
  theme_minimal()
```
Most of the people have 0 affairs and then we have a right skeweness.

```{r}
ggplot(Affairs, aes(x = affairs, fill = gender)) +
  geom_bar(position = "dodge") +
  labs(title = "Distribution of Number of Affairs per Gender",
       x = "Number of Affairs",
       y = "Count") +
  theme_minimal() +
  scale_fill_brewer(palette = "Pastel1") +
  guides(fill = guide_legend(title = "Gender"))
```

The dodging bar plot displays the distribution of the number of affairs, separated by gender:

1. For both genders, the majority have reported having no affairs, as represented by the tallest bars.
2. In the instances of one or more affairs, the counts for males and females are displayed side by side for each number of affairs, allowing for easy comparison.
3. The female counts are represented by the lighter shade bars, while the male counts are in the darker shade.


```{r}
ggplot(Affairs, aes(x=yearsmarried)) +
  geom_histogram(bins=20, alpha=0.7, aes(y=..density..)) 
  labs(title='Distribution of Years Married', x='Years Married', y='Density')

```
```{r}
ggplot(Affairs, aes(x=age)) +
  geom_histogram(bins=20, alpha=0.7, aes(y=..density..)) +
  labs(title='Distribution of Age', x='Age', y='Density')
```


\item[(b)] Suppose we want to explore the characteristics of participants who engage in extramarital sexual intercourse (i.e. affairs). Instead of modeling the number of affairs, consider the binary outcome  - had an affair versus didn't have an affair. Create a new variable to capture this response variable of interest. What might the advantages and disadvantages of this approach to modeling the data be in this context?

```{r}
binary_affairs <- Affairs %>%
  mutate(cheated = case_when(
    affairs == 0 ~ 0,
    .default = 1
  )) %>%
  mutate(gender_binary = case_when(
    gender == "female" ~ 1,
    .default = 0
  )) %>%
  mutate(children_binary = case_when(
    children == "yes" ~ 1,
    .default = 0
  ))

summary(binary_affairs$cheated)
```


The advantages: 
- We will have more data for each outcome as the binary variable of having an affair will have the data from the affairs variable having more than 0.
-Simplicity of model
The disadvantages: 
- We will lose some information which is inherited by the amount of times people have cheated. 
- Bias the model due to oversimplification

\item[(c)] Use an appropriate regression model to explore the relationship between having an affair and other personal characteristics. Comment on which covariates seem to be predictive of having an affair and which do not.

I will use logistic regression.

```{r}
library(corrplot)

# Calculate the correlation matrix
correlation_matrix <- cor(binary_affairs[, c("age", "yearsmarried", 
                                             "children_binary", "religiousness", 
                                             "education", "rating", 
                                             "gender_binary")])

# Create a correlation matrix plot using corrplot
corrplot(correlation_matrix, method = "color", 
         order = "hclust", tl.col = "black")

```

There might be multicolinearity between yearsmarried and age, and also yearsmarried and children.

```{r, linear model}
mylogit <- glm(cheated ~ age + yearsmarried + children_binary + religiousness + 
                 education + rating + gender_binary, data = binary_affairs, 
               family = "binomial")

summary(mylogit)

```
The variables which seem important to predict wheter a person is going to have an affair are: rating and religiousness, as well as yearsmarriedd and age. Age might have multicolinearity with yearsmarried though. 


\item[(d)] Use an all subsets model selection procedure to obtain a "best" fit model. Is the model different from the full model you fit in part (c)? Which variables are included in the "best" fit model? You might find the \texttt{bestglm()} function available in the \texttt{bestglm} package helpful.


```{r}
df <- Affairs
df$had_affair <- as.integer(df$affairs > 0)

df$gender_binary <- ifelse(df$gender == "male", 1, 0)
df$children_binary <- ifelse(df$children == "yes", 1, 0)

df <- df %>% dplyr::select(-c(gender, children, affairs))

full_model <- glm(had_affair ~ ., data = df, family = binomial)

# Stepwise model selection using AIC
best_model <- stepAIC(full_model, direction = "both", trace = 1)

# Print the summary of the best model
summary(best_model)

```

The best model, according to the AIC criterion, includes the following variables:

age
gender_male
rating
religiousness
yearsmarried
The AIC value for this model is approximately 
623.86

\item[(e)] Interpret the model parameters using the model from part (d). 

We can see now that rating, religiously, and yearsmarried p-values are statistically significant and while gender_binary is not, it allows us to define the model depnding if we are measuring a man or a female. 

For every one increase in age there is a decrease by about 4.3% 
Being male increases the log odds of having an affair by 0.38612
For every one-unit increase in rating the log odds of having an affair decrease by 0.4672
For every one-unit increase in religiousness the log odds of having an affair decrease by 0.3271.
For every one-year increase in the duration of marriage the log odds of having an affair increase by 0.1113

```{r}
best_model_for_pred <- glm(cheated ~ age + yearsmarried + religiousness + 
                             rating + gender_binary, data = binary_affairs, 
               family = "binomial")
```


\item[(f)] Create an artificial test dataset where martial rating varies from 1 to 5 and all other variables are set to their means. Use this test dataset and the \texttt{predict} function to obtain predicted probabilities of having an affair for case in the test data. Interpret your results and use a visualization to support your interpretation.

```{r}
mean_values <- colMeans(binary_affairs[, c('age', 'yearsmarried', 'religiousness')])

# Number of samples
n_samples <- 100

# Create a DataFrame with 100 cases
test_data <- data.frame(
  age = rep(mean_values['age'], n_samples),
  yearsmarried = rep(mean_values['yearsmarried'], n_samples),
  religiousness = rep(mean_values['religiousness'], n_samples),
  rating = rep(1:5, each = n_samples / 5),
  gender_binary = sample(0:1, n_samples, replace = TRUE)
)

head(test_data)
```

Doing predictions
```{r}
predicted_probs <- predict(best_model_for_pred, newdata = test_data, type = "response")
threshold <- 0.5
predictions <- ifelse(predicted_probs > threshold, 1, 0)
predictions
```
Making visualization
```{r}
test_data$predicted_probs <- predicted_probs
test_data$predictions <- predictions


plot <- ggplot(test_data, aes(x=rating, y=predicted_probs, color=as.factor(predictions))) +
  geom_point(alpha=0.6) +
  labs(title="Predicted Probabilities of Having an Affair by Marital Rating",
       x="Marital Rating", 
       y="Predicted Probability",
       color="Predicted Affair") +
  theme_minimal()

print(plot)

```


\item[(g)] Reflect on your analysis in this problem. After completing all the parts of this analysis what remaining and additional ethical and privacy concerns do you have?

It is hard to predict how models are going to be used. In this case I feel like the premise of men cheat more than female could lead to potential problems related to rights in case there is a divorce and the model is used in court to see the probability of the person comiting some crimes. This could also create some insurance problems if assets are insured by a man and he has family. This models should be used carefully and with an intention of research more than application. 

\eitem

\newpage

\noindent {\bf Problem 2}  \hfill (10 pts)
\vspace{.25in}

\noindent In this problem we will revisit the \texttt{state} dataset. This data, available as part of the base \textbf{R} package, contains various data related to the 50 states of the United States of America.

\noindent Suppose you want to explore the relationship between a state's \texttt{Murder} rate and other characteristics of the state, for example population, illiteracy rate, and more. Follow the questions below to perform this analysis.

```{r}
data(state) # load data
# Create a new object to store data
states <- data.frame(state.x77)
str(states)
```

```{r}
summary(states)
```
There are no nulls in the data.


\bitem
\item[(a)] Examine the bivariate relationships present in the data. Briefly discuss notable results. You might find the \texttt{scatterplotMatrix()} function available in the \texttt{car} package helpful.

```{r}
library(car)
scatterplotMatrix(states)

```

\item[(b)] Fit a multiple linear regression model. How much variance in the murder rate across states do the predictor variables explain?
```{r}
linear_model <- lm(Murder ~ Population + Income + Illiteracy + Life.Exp + 
                     HS.Grad + Frost + Area, data = states)
summary(linear_model)

```
With our Adjusted R squared we can see that 77.63% of the variance is explained by the variables. 


\item[(c)] Evaluate the statistical assumptions in your regression analysis from part (b) by performing a basic analysis of model residuals and any unusual observations. Discuss any concerns you have about your model.

```{r}
# Get model residuals
residuals <- residuals(linear_model)

# Create a data frame with residuals and predictor variables
residual_data <- data.frame(Residuals = residuals, Predictor = fitted(linear_model))

# Create a scatter plot of residuals against predictor variables
ggplot(residual_data, aes(x = Predictor, y = Residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Residuals vs. Fitted Values",
       x = "Fitted Values",
       y = "Residuals") +
  theme_minimal()


```

Residuals are not in a shape so we can say it 

\item[(d)] Use a stepwise model selection procedure of your choice to obtain a "best" fit model. Is the model different from the full model you fit in part (b)? If yes, how so?

```{r}
# Stepwise model selection using AIC
best_model_lm <- stepAIC(linear_model, direction = "both", trace = 1)
summary(best_model_lm)
```
**Answer**
The model is the same as in the beginning. 

\item[(e)] Assess the model (from part (d)) generalizability. Perform a 10-fold cross validation to estimate model performance. Report the results.

```{r}
library(caret)
# Define the predictor variables (X) and outcome variable (y)
X <- states[, !colnames(states) %in% c("Murder")]
y <- states$Murder

# Specify the train control settings for 10-fold cross-validation
ctrl <- trainControl(method = "cv", number = 10)

# Train a model using your chosen algorithm (replace "your_algorithm" with your chosen algorithm)
model <- train(X, y, method = "lm", trControl = ctrl)

# Print the results and summary of the cross-validation
print(model)

```


I created a model using caret package which is a learning library in R standing for Classification and Regression Training. Allowing for flexibility o train different algorithms using k folds as well. With this model 81.51 percent of the variance is predicted by the variables we are using. 

```{r}
# Load the required libraries
library(tidyverse)
library(MASS)
library(caret) # For 'createFolds' function

# Load your dataset (replace "your_dataset.csv" with your dataset file)

# Define the predictor variables (X) and outcome variable (y)
X <- states[, !colnames(states) %in% c("Murder")]
y <- states$Murder

# Number of folds for cross-validation
num_folds <- 10

# Create indices for cross-validation
set.seed(123) # For reproducibility
folds <- createFolds(y, k = num_folds)

# Initialize a vector to store RMSE for each fold
rmse_results <- numeric(num_folds)

# Perform 10-fold cross-validation
for (i in 1:num_folds) {
  train_indices <- unlist(folds[-i])
  test_indices <- unlist(folds[i])
  model <- lm(y[train_indices] ~ ., data = X[train_indices, ])
  predictions <- predict(model, newdata = X[test_indices, ])
  
  # Compute RMSE for this fold
  rmse_results[i] <- sqrt(mean((predictions - y[test_indices])^2))
}

print(rmse_results)

```


\eitem

\newpage

\noindent {\bf Problem 3} \hfill (5 pts)
\vspace{.25in}

\noindent The Wisconsin Breast Cancer dataset is available as a comma-delimited text file on the UCI Machine Learning Repository \url{http://archive.ics.uci.edu/ml}. Our goal in this problem will be to predict whether observations (i.e. tumors) are malignant or benign. 

```{r}
# Web location of data file
loc <- "http://archive.ics.uci.edu/ml/machine-learning-databases/"
ds <- "breast-cancer-wisconsin/breast-cancer-wisconsin.data"
url <- paste(loc, ds, sep="")
# Load and name data
breast.data <- read.table(url, sep=",", header=FALSE,na.strings = "?")

breast.data 
summary(breast.data)
```

```{r}

names(breast.data) <- c("ID","clumpThickness","sizeUniformity",
"shapeUniformity","marginalAdhesion",
"singleEpithelialCellSize","bareNuclei",
"blandCromatin","normalNucleoli","mitosis",
"class")
# Make response variable into a factor

breast.data$class <- factor(breast.data$class, levels=c(2,4),
labels=c("benign","malignant"))
# Remove ID column
breast <- breast.data[,-1]

head(breast.data)
summary(breast.data)
```

The dataset contains 699 rows and 12 columns. It has different features for medical terms related to breast data.Ten real-valued features are computed for each cell nucleus. Ther are 458 benign out of the 699. This data set capttures cell attributes used for diagnosing brast cancer. We have NAs for bareNuclei, 16 missing values. 



\bitem
\item[(a)] Obtain the data, and load it into \textbf{R} by pulling it directly from the web. (Do \textbf{not} download it and import it from a CSV file.) Give a brief description of the data.

\item[(b)] Tidy the data, ensuring that each variable is properly named and cast as the correct data type. Discuss any missing data.
There are NAs for bareNuclei. 14 of them are benign and 2 are malignant. We could use KNNs to handle missing data. I will just delete those rows because the distribution of benign against malign of that sample looks like the distributino of the 458 out of 699. 

\item[(c)] Split the data into a training and validation set such that a random 70\% of the observations are in the training set. 

```{r}
set.seed(42)
train_indices <- sample(1:nrow(breast.data), 0.7 * nrow(breast.data))
train_data <- breast.data[train_indices, ]
validation_data <- breast.data[-train_indices, ]

num_train_samples <- nrow(train_data)
num_validation_samples <- nrow(validation_data)

num_train_samples
num_validation_samples
```

\item[(d)] Fit a regression model to predict whether tissue samples are malignant or benign. Classify cases in the validation set. Compute and discuss the resulting confusion matrix. Be sure to address which of the errors that are identified you consider most problematic in this context.

```{r}
model <- glm(class ~ . - ID, data = train_data, family = "binomial")


predicted_probs <- predict(model, newdata = validation_data, type = "response")
threshold <- 0.5
predictions <- ifelse(predicted_probs > threshold, "malignant", "benign")


confusion_matrix <- table(validation_data$class, predictions)
confusion_matrix
```
The problems here is that we want our False negatives to be as low as possible or to reduce Type II errors. We have here 6 false negatives which can lead to problems. 

\eitem

\newpage

\noindent {\bf Problem 4} \hfill (10 pts)
\vspace{.25in}

\noindent Please answer the questions below by writing a short response. 

\bitem
\item[(a)] Describe three real-life applications in which \emph{classification} might be useful. Describe the response, as well as the predictors. Is the goal in each application inference or predictions? Explain your answer.
Sentiment Analysis in Social Media:
The sentiment of a social media post
Predictors: Text content, language patterns, emojis, hashtags.
This helps companies understand public opinion about their products and services, enabling them to tailor their strategies accordingly.

Disease Diagnosis from Medical Images:
Presence or absence of a disease in a medical image (e.g., X-ray, MRI).
Predictors: Pixel values, textures, shapes extracted from medical images.
Doctors use this to aid in diagnosing diseases like cancer, enabling earlier detection and better patient outcomes.

Customer Churn Prediction:
Whether a customer will churn
Predictors: Customer demographic information, usage patterns, customer service interactions, billing history.
Companies aim to proactively retain customers by identifying those at risk of leaving and implementing strategies to keep them so the goal is prediction. 

\item[(b)] Describe three real-life applications in which \emph{regression} might be useful. Describe the response, as well as the predictors. Is the goal in each application inference or predictions? Explain your answer. 
Stock Market Index Prediction:
The value of a stock market index on a future date.
Predictors: Historical stock prices, trading volume, economic indicators, market sentiment, and possibly external events.
Prediction for traders and investors use this to make informed decisions about buying, selling, or holding stocks.

House Price Prediction: 
Regression is used to predict the price of a house. The response is the continuous price value.
Predictors:  square footage, number of bedrooms, location, etc. 
The goal is prediction, as it helps potential buyers or sellers estimate house prices.

Energy Consumption Forecasting:
Predicted energy consumption for a specific region over a future period.
Predictors: Historical energy usage, weather data, day of the week, time of day, holidays.
Prediction of utility companies to use this to manage energy production, distribution, and pricing, ensuring a stable energy supply.

\item[(c)] What are the advantages and disadvantages of a very flexible (versus a less flexible) approach for regression or classification? Under what circumstances might a more flexible approach be preferred to a less flexible approach? When might a less flexible approach be preferred?
- Everything depends on the context and environment your model will interact. A flexible approach for regression can adapt to handle a wide range of predictor-response relationships but they can also integrate noise in the training data which might lead too generalization when trying to use new data. We would prefer to use less flexible models when dealing with medical or legal contexts where we cannot be flexible in the outcome. We also have to be less flexible when the dataset is small. But if the dataset is large and we have high dimensional predictors or complex patterns in data then we can more flexible and allow for creativity. The same way a human would decide on day to day interactions. What is not very risky can be flexible.



\eitem

\newpage

\noindent {\bf Problem 5} \hfill (10 pts)
\vspace{.25in}

\noindent Suppose we have a dataset with five predictors, $X_1 =$ GPA, $X_2 =$ IQ, $X_3 =$ Gender (1 for Female, and 0 for Male), $X_4 =$ Interaction between GPA and IQ, and $X_5 =$ Interaction between GPA and Gender. The response is starting salary after graduation (in thousands of dollars). Suppose we use least squares to fit the model and get $\hat{\beta}_0=50, \hat{\beta}_1=20, \hat{\beta}_2=0.07, \hat{\beta}_3=35, \hat{\beta}_4=0.01$, and $\hat{\beta}_5=-10$. 

```{r}
gpa <- 0
IQ <- 0
gender <- 1
interaction_gpa_iq <- 0

50 + 20 * gpa + 0.07 * IQ + 35 * gender + 0.01 * interaction_gpa_iq
```

\bitem
\item[(a)] Which answer is correct and why?

\bitem
\item[i.] For a fixed value of IQ and GPA, males earn more on average than females.
\item[ii.] For a fixed value of IQ and GPA, females earn more on average than males.
\item[iii.] For a fixed value of IQ and GPA, males earn more on average than females provided that
the GPA is high enough.
\item[iv.] For a fixed value of IQ and GPA, females earn more on average than males provided that
the GPA is high enough.

ii is the correct. Solely the fact that a person is a female would gett 35 thousand more dollars. This is based on the model shown above. 

\eitem

\item[(b)] Predict the salary of a female with IQ of 110 and a GPA of 4.0.

```{r}
gpa <- 4.0
IQ <- 110
gender <- 1
interaction_gpa_iq <- 4 * 110

50 + 20 * gpa + 0.07 * IQ + 35 * gender + 0.01 * interaction_gpa_iq
```

The salary would be 172 thousand dollars for that person. 

\item[(c)] True or false: Since the coefficient for the GPA/IQ interaction term is very small, there is little evidence of an interaction effect. Justify your answer.
\eitem

False. We need to determine if the interaction is statistically significant by looking at the p-value. However the effect size might not have practical significance. In the previoous case, we have that interaction effect gave 5k more than if we were not using an interaction effect predictor.
